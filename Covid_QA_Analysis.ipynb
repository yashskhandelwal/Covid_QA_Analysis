{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a8b3bc",
   "metadata": {},
   "source": [
    "# COVID QA Analysis\n",
    "## Advanced Statistical NLP (CSE 291-3) \n",
    "\n",
    "### Yash Khandelwal, Kaushik Ravindran\n",
    "\n",
    "github: https://github.com/yashskhandelwal/Covid_QA_Analysis\n",
    "\n",
    "\n",
    "\n",
    "#### Expected outputs per model:\n",
    "\n",
    "##### Evaluation:\n",
    "*   Exact Match\n",
    "*   F1 Score\n",
    "\n",
    "##### Training time:\n",
    "*   Time taken to fine tune the model\n",
    "*   Average prediction time\n",
    "\n",
    "##### Environmental impact:\n",
    "*   GPU Details\n",
    "*   CO2 emission impact of trainnig the model\n",
    "\n",
    "#### List of models\n",
    "\n",
    "*   BERT: Base, Large\n",
    "*   RoBERTa: Base, Large\n",
    "*   DistilBERT: Base\n",
    "*   ALBERT: Base, XXL\n",
    "*   ELECTRA: Base\n",
    "*   LongFormer: Base, Large\n",
    "*   BigBird: base\n",
    "\n",
    "#### Main libraries:\n",
    "\n",
    "*   pyTorch\n",
    "*   trasnformers (HuggingFace)\n",
    "*   tokenizers (HuggingFace)\n",
    "*   datasets (HuggingFace)\n",
    "*   codecarbon\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27284b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# env setup\n",
    "# install relavant libraries\n",
    "!pip install datasets transformers\n",
    "!pip install accelerate\n",
    "!pip install humanize\n",
    "!pip install millify\n",
    "!pip install tqdm\n",
    "!pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a559a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math, statistics, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from codecarbon import EmissionsTracker\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1785779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /Users/yashkhandelwal/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# login to hugging face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ccb020",
   "metadata": {},
   "source": [
    "### Section 1: Prepping the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ac9bc",
   "metadata": {},
   "source": [
    "##### Section 1.1: load covid qa dataset and get a bearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea4dfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset covid_qa_deepset (/Users/yashkhandelwal/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/fb886523842e312176f92ec8e01e77a08fa15a694f5741af6fc42796ee9c8c46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8691bafe508e4b37b588fbab0aa27a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"covid_qa_deepset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a74e4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document_id', 'context', 'question', 'is_impossible', 'id', 'answers'],\n",
       "        num_rows: 2019\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60729cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': Value(dtype='int32', id=None),\n",
       " 'context': Value(dtype='string', id=None),\n",
       " 'question': Value(dtype='string', id=None),\n",
       " 'is_impossible': Value(dtype='bool', id=None),\n",
       " 'id': Value(dtype='int32', id=None),\n",
       " 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6110f",
   "metadata": {},
   "source": [
    "##### Section 1.2: Print some basic stats for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7209414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average context length is: 32051.449232293213\n",
      "Max context length is: 70846\n",
      "Min context length is: 2876\n",
      "Median context length is: 29857\n"
     ]
    }
   ],
   "source": [
    "#About context lengths\n",
    "context_lengths = list(map(len, raw_datasets['train']['context']))\n",
    "print('Average context length is:', statistics.mean(context_lengths))\n",
    "print('Max context length is:', max(context_lengths))\n",
    "print('Min context length is:', min(context_lengths))\n",
    "print('Median context length is:', statistics.median(context_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74dff950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average question length is: 58.48588410104012\n",
      "Max question length is: 194\n",
      "Min question length is: 11\n",
      "Median question length is: 55\n"
     ]
    }
   ],
   "source": [
    "#About questions lengths\n",
    "question_lengths = list(map(len, raw_datasets['train']['question']))\n",
    "print('Average question length is:', statistics.mean(question_lengths))\n",
    "print('Max question length is:', max(question_lengths))\n",
    "print('Min question length is:', min(question_lengths))\n",
    "print('Median question length is:', statistics.median(question_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32575e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average answer count is: 1\n",
      "Max answer count is: 1\n",
      "Min answer count is: 1\n",
      "Median answer count is: 1\n"
     ]
    }
   ],
   "source": [
    "#About num of answers per question\n",
    "answer_count = list(map(lambda x: len(x['answers']['text']), raw_datasets['train']))\n",
    "print('Average answer count is:', statistics.mean(answer_count))\n",
    "print('Max answer count is:', max(answer_count))\n",
    "print('Min answer count is:', min(answer_count))\n",
    "print('Median answer count is:', statistics.median(answer_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a7f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average answer length is: 93.31698860822189\n",
      "Max answer length is: 933\n",
      "Min answer length is: 1\n",
      "Median answer length is: 65\n"
     ]
    }
   ],
   "source": [
    "#About length of answers\n",
    "answer_lengths = list(map(lambda x: len(x['answers']['text'][0]), raw_datasets['train']))\n",
    "print('Average answer length is:', statistics.mean(answer_lengths))\n",
    "print('Max answer length is:', max(answer_lengths))\n",
    "print('Min answer length is:', min(answer_lengths))\n",
    "print('Median answer length is:', statistics.median(answer_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207bb100",
   "metadata": {},
   "source": [
    "##### Section 1.3: Split dataset into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f2961f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/yashkhandelwal/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/fb886523842e312176f92ec8e01e77a08fa15a694f5741af6fc42796ee9c8c46/cache-7699b1bdb0a55cfe.arrow and /Users/yashkhandelwal/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/fb886523842e312176f92ec8e01e77a08fa15a694f5741af6fc42796ee9c8c46/cache-969d64e58f6c8316.arrow\n"
     ]
    }
   ],
   "source": [
    "raw_datasets_split = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=42)\n",
    "raw_datasets_split['validation'] = raw_datasets_split.pop('test')\n",
    "raw_datasets = raw_datasets_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87ea6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document_id', 'context', 'question', 'is_impossible', 'id', 'answers'],\n",
       "        num_rows: 1817\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document_id', 'context', 'question', 'is_impossible', 'id', 'answers'],\n",
       "        num_rows: 202\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490c423",
   "metadata": {},
   "source": [
    "#### Section 2: Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d85d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model_checkpoint = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ddc3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9625be",
   "metadata": {},
   "source": [
    "###### Section 2.1 Preprocessing raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39068739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split long context into multiple features \n",
    "# find answer start and end token id in each of the features\n",
    "def preprocess_training_examples(examples):\n",
    "    #overlapping between context split in multiple features\n",
    "    stride = 50\n",
    "\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    context =  examples[\"context\"]\n",
    "    answers = examples[\"answers\"] \n",
    "    \n",
    "    #use model tokenizer to tokenize examples\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    #return_overflowing_tokens -- for each feature, it represents the original example it belonged to\n",
    "    #return_offsets_mapping -- for each token, it returns the start and end position of the word represented by that token in the original context\n",
    "    \n",
    "        \n",
    "    #pop offset_mapping and overflow_to_sample mapping\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    #map the start an dend token of answer in each feature\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    #for each feature\n",
    "    for i, offset in enumerate(offset_mapping): \n",
    "        sample_idx = sample_map[i] #get original example index\n",
    "        answer = answers[sample_idx] #get the answer for that example\n",
    "        start_char = answer[\"answer_start\"][0] #start char of answer in original context\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0]) #end char of answer in original context\n",
    "        \n",
    "        #labels in tokenized input indicating whether token belongs to question (0), context (1), or special token (None)\n",
    "        sequence_ids = inputs.sequence_ids(i) \n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2988c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yashkhandelwal/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/fb886523842e312176f92ec8e01e77a08fa15a694f5741af6fc42796ee9c8c46/cache-a6cb04fbadc16da1.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e192c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6712b3",
   "metadata": {},
   "source": [
    "#### Section 3: Setting up evaluation for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "741ffccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17e741be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3150b",
   "metadata": {},
   "source": [
    "#### Section 4: Finetuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e52b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a training loop\n",
    "def current_milli_time():\n",
    "    return round(time.time() * 1000)\n",
    "\n",
    "def finetune_model(model, args, train_dataset, val_dataset, tokenizer):\n",
    "    from transformers import Trainer\n",
    "    from codecarbon import EmissionsTracker\n",
    "    import torch, time\n",
    "\n",
    "    tracker = EmissionsTracker()\n",
    "    tracker.start()\n",
    "    start_time = current_milli_time()\n",
    "\n",
    "    trainer = Trainer(\n",
    "      model=model,\n",
    "      args=args,\n",
    "      train_dataset=train_dataset,\n",
    "      eval_dataset=None,\n",
    "      tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    emissions = tracker.stop()\n",
    "    print('Emissions:', emissions, 'CO_2 eq (in KG)')\n",
    "    if torch.cuda.is_available():\n",
    "        print('GPU device name:', torch.cuda.get_device_properties(0).name)\n",
    "        print('GPU device memory:', torch.cuda.get_device_properties(0).total_memory/(10**9), \"GiB\")\n",
    "    print('Training time:', (current_milli_time()-start_time)/(1000*60))\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32bc808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/yashkhandelwal/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/yashkhandelwal/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "CODECARBON : Failed to match CPU TDP constant. Falling back on a global constant.\n",
      "/Users/yashkhandelwal/Desktop/CSE_291_NLP/covid_qa_analysis_bert_base is already a clone of https://huggingface.co/armageddon/covid_qa_analysis_bert_base. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "CODECARBON : Failed to match CPU TDP constant. Falling back on a global constant.\n",
      "***** Running training *****\n",
      "  Num examples = 9\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 01:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to covid_qa_analysis_bert_base/checkpoint-2\n",
      "Configuration saved in covid_qa_analysis_bert_base/checkpoint-2/config.json\n",
      "Model weights saved in covid_qa_analysis_bert_base/checkpoint-2/pytorch_model.bin\n",
      "tokenizer config file saved in covid_qa_analysis_bert_base/checkpoint-2/tokenizer_config.json\n",
      "Special tokens file saved in covid_qa_analysis_bert_base/checkpoint-2/special_tokens_map.json\n",
      "tokenizer config file saved in covid_qa_analysis_bert_base/tokenizer_config.json\n",
      "Special tokens file saved in covid_qa_analysis_bert_base/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emissions: 0.00039581337392363214 CO_2 eq (in KG)\n",
      "Training time: 2.7594333333333334\n"
     ]
    }
   ],
   "source": [
    "# set model and training arguments\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(pre_trained_model_checkpoint)\n",
    "args = TrainingArguments(\n",
    "    \"covid_qa_analysis_bert_base\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    hub_model_id=\"armageddon/covid_qa_analysis_bert_base\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "# finetune model\n",
    "trainer = finetune_model(model, args, train_dataset, None, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba1c8a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to covid_qa_analysis_bert_base\n",
      "Configuration saved in covid_qa_analysis_bert_base/config.json\n",
      "Model weights saved in covid_qa_analysis_bert_base/pytorch_model.bin\n",
      "tokenizer config file saved in covid_qa_analysis_bert_base/tokenizer_config.json\n",
      "Special tokens file saved in covid_qa_analysis_bert_base/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224b688da97d45969e31a6be165763a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Feb23_11-15-41_Yashs-MacBook-Pro.local/events.out.tfevents.1645643748.Yashs-MacBook-Pro.local…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/armageddon/covid_qa_analysis_bert_base\n",
      "   9ad215c..701e101  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'covid_qa_deepset', 'type': 'covid_qa_deepset', 'args': 'covid_qa_deepset'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/armageddon/covid_qa_analysis_bert_base/commit/701e101baf9d4d474f245806695951135538570f'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push to github if needed\n",
    "trainer.push_to_hub(commit_message=\"Working Code\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2scf",
   "language": "python",
   "name": "l2scf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
