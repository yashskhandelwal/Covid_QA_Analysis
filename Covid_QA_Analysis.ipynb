{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a8b3bc",
   "metadata": {},
   "source": [
    "# COVID QA Analysis (BERT-Base)\n",
    "## Advanced Statistical NLP (CSE 291-3) \n",
    "### Yash Khandelwal, Kaushik Ravindran\n",
    "\n",
    "github: https://github.com/yashskhandelwal/Covid_QA_Analysis\n",
    "\n",
    "#### Expected outputs per model:\n",
    "\n",
    "##### Evaluation:\n",
    "*   Exact Match\n",
    "*   F1 Score\n",
    "\n",
    "##### Training time:\n",
    "*   Time taken to fine tune the model\n",
    "*   Average prediction time\n",
    "\n",
    "##### Environmental impact:\n",
    "*   GPU Details\n",
    "*   CO2 emission impact of training the model\n",
    "\n",
    "#### List of models\n",
    "\n",
    "*   BERT: Base, Large\n",
    "*   RoBERTa: Base, Large\n",
    "*   DistilBERT: Base\n",
    "*   ALBERT: Base, XXL\n",
    "*   ELECTRA: Base\n",
    "*   LongFormer: Base, Large\n",
    "*   BigBird: base\n",
    "\n",
    "#### Main libraries:\n",
    "\n",
    "*   pyTorch\n",
    "*   trasnformers (HuggingFace)\n",
    "*   tokenizers (HuggingFace)\n",
    "*   datasets (HuggingFace)\n",
    "*   codecarbon\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27284b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# env setup\n",
    "# install relavant libraries\n",
    "!pip install datasets transformers\n",
    "!pip install accelerate\n",
    "!pip install humanize\n",
    "!pip install millify\n",
    "!pip install tqdm\n",
    "!pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a559a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math, statistics, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from codecarbon import EmissionsTracker\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to hugging face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "dataset = \"covid_qa_deepset\"\n",
    "pre_trained_model_checkpoint = \"bert-base-cased\"\n",
    "model_name = \"covid_qa_analysis_bert_base\"\n",
    "hub_model_id = \"armageddon/covid_qa_analysis_bert_base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ccb020",
   "metadata": {},
   "source": [
    "### Section 1: Prepping the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ac9bc",
   "metadata": {},
   "source": [
    "##### Section 1.1: load covid qa dataset and get a bearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60729cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6110f",
   "metadata": {},
   "source": [
    "##### Section 1.2: Print some basic stats for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7209414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# about context lengths\n",
    "context_lengths = list(map(len, raw_datasets['train']['context']))\n",
    "print('Average context length is:', statistics.mean(context_lengths))\n",
    "print('Max context length is:', max(context_lengths))\n",
    "print('Min context length is:', min(context_lengths))\n",
    "print('Median context length is:', statistics.median(context_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dff950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# about questions lengths\n",
    "question_lengths = list(map(len, raw_datasets['train']['question']))\n",
    "print('Average question length is:', statistics.mean(question_lengths))\n",
    "print('Max question length is:', max(question_lengths))\n",
    "print('Min question length is:', min(question_lengths))\n",
    "print('Median question length is:', statistics.median(question_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32575e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#About num of answers per question\n",
    "answer_count = list(map(lambda x: len(x['answers']['text']), raw_datasets['train']))\n",
    "print('Average answer count is:', statistics.mean(answer_count))\n",
    "print('Max answer count is:', max(answer_count))\n",
    "print('Min answer count is:', min(answer_count))\n",
    "print('Median answer count is:', statistics.median(answer_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a7f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#About length of answers\n",
    "answer_lengths = list(map(lambda x: len(x['answers']['text'][0]), raw_datasets['train']))\n",
    "print('Average answer length is:', statistics.mean(answer_lengths))\n",
    "print('Max answer length is:', max(answer_lengths))\n",
    "print('Min answer length is:', min(answer_lengths))\n",
    "print('Median answer length is:', statistics.median(answer_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207bb100",
   "metadata": {},
   "source": [
    "##### Section 1.3: Split dataset into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2961f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets_split = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=42)\n",
    "raw_datasets_split['validation'] = raw_datasets_split.pop('test')\n",
    "raw_datasets = raw_datasets_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490c423",
   "metadata": {},
   "source": [
    "#### Section 2: Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9625be",
   "metadata": {},
   "source": [
    "###### Section 2.1 Preprocessing raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39068739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing for training \n",
    "# split long context into multiple features \n",
    "# find answer start and end token id in each of the features\n",
    "def preprocess_training_examples(examples):\n",
    "    #overlapping between context split in multiple features\n",
    "    stride = 50\n",
    "\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    context =  examples[\"context\"]\n",
    "    answers = examples[\"answers\"] \n",
    "    \n",
    "    # use model tokenizer to tokenize examples\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # return_overflowing_tokens -- for each feature, it represents the original example it belonged to\n",
    "    # return_offsets_mapping -- for each token, it returns the start and end position of the word represented by that token in the original context\n",
    "        \n",
    "    # pop offset_mapping and overflow_to_sample mapping\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    # map the start an dend token of answer in each feature\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    # for each feature\n",
    "    for i, offset in enumerate(offset_mapping): \n",
    "        sample_idx = sample_map[i] # get original example index\n",
    "        answer = answers[sample_idx] # get the answer for that example\n",
    "        start_char = answer[\"answer_start\"][0] # start char of answer in original context\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0]) # end char of answer in original context\n",
    "        \n",
    "        # labels in tokenized input indicating whether token belongs to question (0), context (1), or special token (None)\n",
    "        sequence_ids = inputs.sequence_ids(i) \n",
    "\n",
    "        # find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing for validation examples\n",
    "def preprocess_validation_examples(examples):\n",
    "    stride = 50\n",
    "\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    context =  examples[\"context\"]\n",
    "    answers = examples[\"answers\"] \n",
    "    \n",
    "    # use model tokenizer to tokenize examples\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # return_overflowing_tokens -- for each feature, it represents the original example it belonged to\n",
    "    # return_offsets_mapping -- for each token, it returns the start and end position of the word represented by that token in the original context\n",
    "    \n",
    "    # pop overflow_to_sample mapping\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i] # get original example index\n",
    "        example_ids.append(examples[\"id\"][sample_idx]) # get and store the id of the original sample index\n",
    "        \n",
    "        # labels in tokenized input indicating whether token belongs to question (0), context (1), or special token (None)\n",
    "        sequence_ids = inputs.sequence_ids(i)  \n",
    "        \n",
    "        # update offset mapping so that only context offset mapping is stored and question offset mapping is discarded\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "    \n",
    "    # add a new column to inputs and return\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2988c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e192c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b584799",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cdb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00490505",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6712b3",
   "metadata": {},
   "source": [
    "#### Section 3: Setting up evaluation for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9597681",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "metric = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e741be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3150b",
   "metadata": {},
   "source": [
    "#### Section 4: Finetuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_milli_time():\n",
    "    return round(time.time() * 1000)\n",
    "\n",
    "# define a training loop\n",
    "def finetune_model(model, args, train_dataset, val_dataset, tokenizer):\n",
    "    from transformers import Trainer\n",
    "    from codecarbon import EmissionsTracker\n",
    "    import torch, time\n",
    "\n",
    "    tracker = EmissionsTracker()\n",
    "    tracker.start()\n",
    "    start_time = current_milli_time()\n",
    "\n",
    "    trainer = Trainer(\n",
    "      model=model,\n",
    "      args=args,\n",
    "      train_dataset=train_dataset,\n",
    "      eval_dataset=None,\n",
    "      tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    emissions = tracker.stop()\n",
    "    print('Emissions:', emissions, 'CO_2 eq (in KG)')\n",
    "    if torch.cuda.is_available():\n",
    "        print('GPU device name:', torch.cuda.get_device_properties(0).name)\n",
    "        print('GPU device memory:', torch.cuda.get_device_properties(0).total_memory/(10**9), \"GiB\")\n",
    "    print('Training time:', (current_milli_time()-start_time)/(1000*60))\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model and training arguments\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(pre_trained_model_checkpoint)\n",
    "args = TrainingArguments(\n",
    "    model_name,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    hub_model_id=hub_model_id,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune model\n",
    "trainer = finetune_model(model, args, train_dataset, None, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbd83b",
   "metadata": {},
   "source": [
    "#### Section 5: validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate on training dataset\n",
    "predictions = trainer.predict(train_valid_dataset)\n",
    "start_logits, end_logits = predictions.predictions\n",
    "\n",
    "print(\"validation metrics on training dataset are as follows:]\\n\",compute_metrics(start_logits, end_logits, train_valid_dataset,raw_datasets[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate on validation dataset\n",
    "predictions = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions.predictions\n",
    "\n",
    "print(\"validation metrics on validation dataset are as follows:]\\n\",compute_metrics(start_logits, end_logits, validation_dataset,raw_datasets[\"validation\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac56f6",
   "metadata": {},
   "source": [
    "#### Section 6: Push model to hugging-face library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to github if needed\n",
    "trainer.push_to_hub(commit_message=\"Run {}\".format(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2scf",
   "language": "python",
   "name": "l2scf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
