{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43a8b3bc",
      "metadata": {
        "id": "43a8b3bc"
      },
      "source": [
        "# Script for model evaluation\n",
        "## COVID-QA Analysis\n",
        "### Yash Khandelwal, Kaushik Ravindran\n",
        "\n",
        "github: https://github.com/yashskhandelwal/Covid_QA_Analysis\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27284b1f",
      "metadata": {
        "id": "27284b1f"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# env setup\n",
        "# install relavant libraries\n",
        "!pip install datasets transformers\n",
        "!pip install accelerate\n",
        "!pip install humanize\n",
        "!pip install millify\n",
        "!pip install tqdm\n",
        "!apt-get install git-lfs\n",
        "!pip install codecarbon\n",
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# for running on tpu\n",
        "!pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "kBNJkRIqgwyp"
      },
      "id": "kBNJkRIqgwyp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a559a40",
      "metadata": {
        "id": "8a559a40"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import math, statistics, time\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "import torch\n",
        "from codecarbon import EmissionsTracker\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1785779e",
      "metadata": {
        "id": "1785779e"
      },
      "outputs": [],
      "source": [
        "# login to hugging face\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d2bda78",
      "metadata": {
        "id": "2d2bda78"
      },
      "outputs": [],
      "source": [
        "# constants\n",
        "dataset = \"covid_qa_deepset\"\n",
        "pre_trained_model_checkpoint = \"twmkn9/bert-base-uncased-squad2\"\n",
        "model_name = \"covid_qa_analysis_bert_base_uncased_squad2\"\n",
        "hub_model_id = \"armageddon/covid_qa_analysis_bert_base_uncased_squad2\"\n",
        "stride = 150\n",
        "max_answer_length=150"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82ccb020",
      "metadata": {
        "id": "82ccb020"
      },
      "source": [
        "### Get the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea4dfd8",
      "metadata": {
        "id": "cea4dfd8"
      },
      "outputs": [],
      "source": [
        "raw_datasets = load_dataset(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split dataset into train and test.\n",
        "raw_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=42)"
      ],
      "metadata": {
        "id": "4UkpF12Oeqcp"
      },
      "id": "4UkpF12Oeqcp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization code section"
      ],
      "metadata": {
        "id": "SIZ0ZJHj6Xo8"
      },
      "id": "SIZ0ZJHj6Xo8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ddc3c9d",
      "metadata": {
        "id": "7ddc3c9d"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c248b01c",
      "metadata": {
        "id": "c248b01c"
      },
      "outputs": [],
      "source": [
        "# pre-processing for validation examples\n",
        "def preprocess_validation_examples(examples):\n",
        "\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    context =  examples[\"context\"]\n",
        "    answers = examples[\"answers\"] \n",
        "    \n",
        "    # use model tokenizer to tokenize examples\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        context,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    # return_overflowing_tokens -- for each feature, it represents the original example it belonged to\n",
        "    # return_offsets_mapping -- for each token, it returns the start and end position of the word represented by that token in the original context\n",
        "    \n",
        "    # pop overflow_to_sample mapping\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = sample_map[i] # get original example index\n",
        "        example_ids.append(examples[\"id\"][sample_idx]) # get and store the id of the original sample index\n",
        "        \n",
        "        # labels in tokenized input indicating whether token belongs to question (0), context (1), or special token (None)\n",
        "        sequence_ids = inputs.sequence_ids(i)  \n",
        "        \n",
        "        # update offset mapping so that only context offset mapping is stored and question offset mapping is discarded\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "    \n",
        "    # add a new column to inputs and return\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99391d68",
      "metadata": {
        "id": "99391d68"
      },
      "outputs": [],
      "source": [
        "train_valid_dataset = raw_datasets[\"train\"].map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        "    load_from_cache_file=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_valid_dataset = raw_datasets[\"test\"].map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"test\"].column_names,\n",
        "    load_from_cache_file=False\n",
        ")"
      ],
      "metadata": {
        "id": "_0Y-xPlEewI5"
      },
      "id": "_0Y-xPlEewI5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Code Section"
      ],
      "metadata": {
        "id": "QIUFkI726SzA"
      },
      "id": "QIUFkI726SzA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b725b1b3",
      "metadata": {
        "id": "b725b1b3"
      },
      "outputs": [],
      "source": [
        "n_best = 20\n",
        "metric = load_metric(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e741be",
      "metadata": {
        "id": "17e741be"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(start_logits, end_logits, features, examples, max_answer_length):\n",
        "    example_to_features = defaultdict(list)\n",
        "    for idx, feature in enumerate(features):\n",
        "        example_to_features[feature[\"example_id\"]].append(idx)\n",
        "\n",
        "    predicted_answers = []\n",
        "    for example in tqdm(examples):\n",
        "        \n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "\n",
        "        # Loop through all features associated with that example\n",
        "        for feature_index in example_to_features[example_id]:\n",
        "          start_logit = start_logits[feature_index]\n",
        "          end_logit = end_logits[feature_index]\n",
        "          offsets = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "          start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "          end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "          for start_index in start_indexes:\n",
        "              for end_index in end_indexes:\n",
        "                  # Skip answers that are not fully in the context\n",
        "                  if offsets[start_index] in [None, []] or offsets[end_index] in [None, []]:\n",
        "                      continue\n",
        "                  # Skip answers with a length that is either < 0 or > max_answer_length\n",
        "                  if (\n",
        "                      end_index < start_index\n",
        "                      or end_index - start_index + 1 > max_answer_length\n",
        "                  ):\n",
        "                      continue\n",
        "\n",
        "                  answer = {\n",
        "                      \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                      \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                  }\n",
        "                  answers.append(answer)\n",
        "\n",
        "        # Select the answer with the best score\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "            predicted_answers.append(\n",
        "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
        "            )\n",
        "        else:\n",
        "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
        "\n",
        "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
        "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b157a03",
      "metadata": {
        "id": "9b157a03"
      },
      "source": [
        "### Validating the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get finetuned model\n",
        "finetuned_model = AutoModelForQuestionAnswering.from_pretrained(hub_model_id, use_auth_token=True)\n",
        "\n",
        "# set training args\n",
        "args = TrainingArguments(\n",
        "    model_name,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=False,\n",
        "    hub_model_id=hub_model_id,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "# create trainer for prediction\n",
        "trainer = Trainer(\n",
        "      model=finetuned_model,\n",
        "      args=args,\n",
        "      train_dataset=None,\n",
        "      eval_dataset=None,\n",
        "      tokenizer=tokenizer,\n",
        "  )"
      ],
      "metadata": {
        "id": "ksG9nhZZvqiN"
      },
      "id": "ksG9nhZZvqiN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate on training dataset\n",
        "predictions_tv = trainer.predict(train_valid_dataset)\n",
        "start_logits_tv, end_logits_tv = predictions_tv.predictions\n",
        "print(\"Metrics on training dataset:\\n\", compute_metrics(start_logits_tv, end_logits_tv, train_valid_dataset, raw_datasets[\"train\"], max_answer_length))"
      ],
      "metadata": {
        "id": "KEvyfyq953F0"
      },
      "id": "KEvyfyq953F0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate on test dataset\n",
        "predictions = trainer.predict(test_valid_dataset)\n",
        "start_logits, end_logits = predictions.predictions\n",
        "print(\"Metrics on test dataset:\\n\", compute_metrics(start_logits, end_logits, test_valid_dataset, raw_datasets[\"test\"], max_answer_length))"
      ],
      "metadata": {
        "id": "fU6Z1RRB2qbD"
      },
      "id": "fU6Z1RRB2qbD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing average response time"
      ],
      "metadata": {
        "id": "d2lM6slU94iz"
      },
      "id": "d2lM6slU94iz"
    },
    {
      "cell_type": "code",
      "source": [
        "# code to find average response time\n",
        "\n",
        "# set training args\n",
        "args = TrainingArguments(\n",
        "    model_name,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=False,\n",
        "    hub_model_id=hub_model_id,\n",
        "    push_to_hub=True,\n",
        "    log_level='critical',\n",
        "    logging_strategy='no',\n",
        "    disable_tqdm=True\n",
        ")\n",
        "\n",
        "# create trainer for prediction\n",
        "trainer = Trainer(\n",
        "      model=finetuned_model,\n",
        "      args=args,\n",
        "      train_dataset=None,\n",
        "      eval_dataset=None,\n",
        "      tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "def current_milli_time():\n",
        "  return round(time.time() * 1000)\n",
        "\n",
        "start_time = current_milli_time()\n",
        "test_count = 500\n",
        "for i in range(test_count):\n",
        "  ds_temp = test_valid_dataset.select([i])\n",
        "  predictions = trainer.predict(ds_temp)\n",
        "total_time = current_milli_time()-start_time\n",
        "\n",
        "print(\"Average response time is:\", total_time/test_count, \"ms\")"
      ],
      "metadata": {
        "id": "S0CcH3Gt7sDg"
      },
      "id": "S0CcH3Gt7sDg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}