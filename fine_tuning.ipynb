{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a8b3bc",
   "metadata": {
    "id": "43a8b3bc"
   },
   "source": [
    "# Script to fine-tune model\n",
    "## COVID-QA Analysis\n",
    "### Yash Khandelwal, Kaushik Ravindran\n",
    "\n",
    "github: https://github.com/yashskhandelwal/Covid_QA_Analysis\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27284b1f",
   "metadata": {
    "id": "27284b1f"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# env setup\n",
    "# install relavant libraries\n",
    "!pip install datasets transformers\n",
    "!pip install accelerate\n",
    "!pip install humanize\n",
    "!pip install millify\n",
    "!pip install tqdm\n",
    "!apt-get install git-lfs\n",
    "!pip install codecarbon\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kBNJkRIqgwyp",
   "metadata": {
    "id": "kBNJkRIqgwyp"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# for running on tpu\n",
    "!pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a559a40",
   "metadata": {
    "id": "8a559a40"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import math, statistics, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "import torch\n",
    "from codecarbon import EmissionsTracker\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785779e",
   "metadata": {
    "id": "1785779e"
   },
   "outputs": [],
   "source": [
    "# login to hugging face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2bda78",
   "metadata": {
    "id": "2d2bda78"
   },
   "outputs": [],
   "source": [
    "# set constants\n",
    "dataset = \"covid_qa_deepset\"\n",
    "pre_trained_model_checkpoint = \"twmkn9/bert-base-uncased-squad2\"\n",
    "model_name = \"bert-base-uncased-squad2-covid-qa-deepset\"\n",
    "hub_model_id = \"armageddon/bert-base-uncased-squad2-covid-qa-deepset\"\n",
    "stride = 150\n",
    "max_answer_length=150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ccb020",
   "metadata": {
    "id": "82ccb020"
   },
   "source": [
    "#### Section 1: Prepping the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4dfd8",
   "metadata": {
    "id": "cea4dfd8"
   },
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2961f7",
   "metadata": {
    "id": "0f2961f7"
   },
   "outputs": [],
   "source": [
    "#Split dataset into train and test.\n",
    "raw_datasets_split = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=42)\n",
    "raw_datasets = raw_datasets_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490c423",
   "metadata": {
    "id": "8490c423"
   },
   "source": [
    "#### Section 2: Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc3c9d",
   "metadata": {
    "id": "7ddc3c9d"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39068739",
   "metadata": {
    "id": "39068739"
   },
   "outputs": [],
   "source": [
    "# pre-processing for training \n",
    "# split long context into multiple features \n",
    "# find answer start and end token id in each of the features\n",
    "def preprocess_training_examples(examples):\n",
    "    #overlapping between context split in multiple features\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    context =  examples[\"context\"]\n",
    "    answers = examples[\"answers\"] \n",
    "    \n",
    "    # use model tokenizer to tokenize examples\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # return_overflowing_tokens -- for each feature, it represents the original example it belonged to\n",
    "    # return_offsets_mapping -- for each token, it returns the start and end position of the word represented by that token in the original context\n",
    "        \n",
    "    # pop offset_mapping and overflow_to_sample mapping\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    # map the start and end token of answer in each feature\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    # for each feature\n",
    "    for i, offset in enumerate(offset_mapping): \n",
    "        sample_idx = sample_map[i] # get original example index\n",
    "        answer = answers[sample_idx] # get the answer for that example\n",
    "        start_char = answer[\"answer_start\"][0] # start char of answer in original context\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0]) # end char of answer in original context\n",
    "        \n",
    "        # labels in tokenized input indicating whether token belongs to question (0), context (1), or special token (None)\n",
    "        sequence_ids = inputs.sequence_ids(i) \n",
    "\n",
    "        # find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2988c74",
   "metadata": {
    "id": "b2988c74"
   },
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3150b",
   "metadata": {
    "id": "afa3150b"
   },
   "source": [
    "#### Section 4: Finetuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52b6f9",
   "metadata": {
    "id": "0e52b6f9"
   },
   "outputs": [],
   "source": [
    "def current_milli_time():\n",
    "    return round(time.time() * 1000)\n",
    "\n",
    "# define a training loop\n",
    "def finetune_model(model, args, train_dataset, val_dataset, tokenizer):\n",
    "    from transformers import Trainer\n",
    "    from codecarbon import EmissionsTracker\n",
    "    import torch, time\n",
    "\n",
    "    tracker = EmissionsTracker()\n",
    "    tracker.start()\n",
    "    start_time = current_milli_time()\n",
    "\n",
    "    trainer = Trainer(\n",
    "      model=model,\n",
    "      args=args,\n",
    "      train_dataset=train_dataset,\n",
    "      eval_dataset=None,\n",
    "      tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    emissions = tracker.stop()\n",
    "    print('Emissions:', emissions, 'CO_2 eq (in KG)')\n",
    "    if torch.cuda.is_available():\n",
    "        print('GPU device name:', torch.cuda.get_device_properties(0).name)\n",
    "        print('GPU device memory:', torch.cuda.get_device_properties(0).total_memory/(10**9), \"GiB\")\n",
    "    print('Training time:', (current_milli_time()-start_time)/(1000*60))\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc808c",
   "metadata": {
    "id": "32bc808c"
   },
   "outputs": [],
   "source": [
    "# set model and training arguments\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(pre_trained_model_checkpoint)\n",
    "args = TrainingArguments(\n",
    "    model_name,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    hub_model_id=hub_model_id,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dbd3c",
   "metadata": {
    "id": "285dbd3c"
   },
   "outputs": [],
   "source": [
    "# finetune model\n",
    "trainer = finetune_model(model, args, train_dataset, None, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92155581",
   "metadata": {
    "id": "92155581"
   },
   "source": [
    "#### Section 5: Push model to hugging-face library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c8a75",
   "metadata": {
    "id": "ba1c8a75"
   },
   "outputs": [],
   "source": [
    "# push to huggingface vcs if needed\n",
    "trainer.push_to_hub(commit_message=\"Train finetuned model checkpoint\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fine-tuning.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
